{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Can the data answer the question?\n",
    "\n",
    "-- DS: optimise customer experience by tailoring cnotent to user needs (identify behaviour patterns)\n",
    "\n",
    "-- ML: difficulty finding content (identify 404 patterns) (identify opportunity to improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding variables\n",
    "\n",
    "Parameters described here: https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring analysis variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "folder_raw_data = '../data/raw'\n",
    "folder_clean_data = '../data/cleaned'\n",
    "symbols_dirty = re.compile(' -0400| -|\\[|\\]')\n",
    "raw_encoding='iso-8859-1'\n",
    "\n",
    "cols_raw_log = ['host', 'timestamp', 'request', 'response', 'bytes']\n",
    "dtype_cols_raw_log = {'host': str, 'timestamp': np.datetime64, 'request': str, 'response': str, 'bytes': np.int64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to clean data from unnecessary dashes so the tsv parsing would be more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_logs(file):\n",
    "    with open(folder_clean_data + '/' + file, 'w', encoding=raw_encoding) as c:\n",
    "        with open(folder_raw_data + '/' + file, 'r', encoding=raw_encoding) as r:\n",
    "            for log_line in r:\n",
    "                log_line = symbols_dirty.sub('', log_line)\n",
    "                c.write(log_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring adherence to data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_type(df_logs):\n",
    "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    df_logs['bytes'] = df_logs['bytes'].fillna(0)\n",
    "    df_logs = df_logs.astype(dtype_cols_raw_log)\n",
    "    return df_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw logs and converting in an expected dataframe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logs():\n",
    "    files = os.listdir(folder_raw_data)\n",
    "    files = [file for file in files if os.path.isfile(folder_raw_data + '/' + file)]\n",
    "    df_logs = pd.DataFrame(columns=cols_raw_log)\n",
    "    for file in files:\n",
    "        clean_raw_logs(file)\n",
    "        df_log = pd.read_csv(folder_clean_data + '/' + file, sep=' ', encoding=raw_encoding, header=None, names=cols_raw_log, \n",
    "                             on_bad_lines='skip') #'warn')\n",
    "        df_log = apply_data_type(df_log)\n",
    "        df_logs = pd.concat([df_logs, df_log], axis=0)\n",
    "    return df_logs\n",
    "\n",
    "df_loaded_logs = load_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded_logs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset and identify features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying important variables in terms of answering the questions:\n",
    "1. *host*: identifying requester (to be used for grouping later).\n",
    "2. *timestamp*: indicator for specific times where experience was bad. \n",
    "    * To be used for grouping per day, hour, minutes. \n",
    "    * Year is irrelevant (as dataset is limited), but will be kept for flexibility.\n",
    "3. *request*: identifying the pages viewed/requested.\n",
    "    * Request method\n",
    "    * URL traversal path\n",
    "    * Page OR Resource OR None\n",
    "    * HTTP/1.0 is irrelevant (doesn't give insight because all requests use it for this dataset)\n",
    "4. *response*: identifies response codes\n",
    "    * nominal: success, redirect, not found\n",
    "    * more indicator may be needed to identify points for improvement. refer to https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "5. *bytes*: marginally irrelevant, except for explaining why certain pages may take a long time to load (however there are no other captured information available to validate this).\n",
    "    * threshold indicator to build case for potential point for improvement: CDN\n",
    "\n",
    "These will then be formed to data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "from http import HTTPStatus\n",
    "\n",
    "folder_clean_data = '../data/cleaned'\n",
    "raw_encoding='iso-8859-1'\n",
    "\n",
    "cols_raw_log = ['host', 'timestamp', 'request', 'response', 'bytes']\n",
    "dtype_cols_raw_log = {'host': str, 'timestamp': np.datetime64, 'request': str, 'response': str, 'bytes': np.int64}\n",
    "\n",
    "def apply_data_type(df_logs):\n",
    "    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format='%d/%b/%Y:%H:%M:%S')\n",
    "    df_logs['bytes'] = df_logs['bytes'].fillna(0)\n",
    "    df_logs = df_logs.astype(dtype_cols_raw_log)\n",
    "    return df_logs\n",
    "\n",
    "def load_cleaned_logs():\n",
    "    files = os.listdir(folder_clean_data)\n",
    "    files = [file for file in files if os.path.isfile(folder_clean_data + '/' + file)]\n",
    "    df_logs = pd.DataFrame(columns=cols_raw_log)\n",
    "    for file in files:\n",
    "        df_log = pd.read_csv(folder_clean_data + '/' + file, sep=' ', encoding=raw_encoding, header=None, names=cols_raw_log, \n",
    "                             on_bad_lines='skip') #'warn')\n",
    "        df_logs = pd.concat([df_logs, df_log], axis=0)\n",
    "    return apply_data_type(df_logs)\n",
    "\n",
    "df_cleaned_logs = load_cleaned_logs()\n",
    "df_cleaned_logs.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host\n",
    "\n",
    "* Insight idea: categorise the type of visitor based on url (e.g. government, education, commercial, etc)\n",
    "* Add Domain column\n",
    "* Insight idea: identify visiting countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visitor_domain(url):\n",
    "    try:\n",
    "        domain_index = url.rindex('.')\n",
    "        domain = str(url[(domain_index + 1) :])\n",
    "        if domain.isdigit():\n",
    "            domain = None\n",
    "    except:\n",
    "        domain = None\n",
    "    return domain\n",
    "\n",
    "def categorise_visitor(domain):\n",
    "    switcher = {\n",
    "        'net': 'network',\n",
    "        'com': 'commercial',\n",
    "        'org': 'organisation',\n",
    "        'edu': 'education',\n",
    "        'gov': 'government',\n",
    "        'int': 'international',\n",
    "    }\n",
    "    domain_length = len(domain)\n",
    "    try:\n",
    "        if domain_length == 2:\n",
    "            category = 'country'\n",
    "        elif domain_length == 3:\n",
    "            category = switcher.get(domain, 'unknown')\n",
    "        else:\n",
    "            category = 'unknown'\n",
    "    except:\n",
    "        category = 'unknown'\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df to show who are the visitors\n",
    "df_visitors = pd.DataFrame(df_cleaned_logs['host'])\n",
    "\n",
    "# identify visitor profile through domain\n",
    "df_visitors['domain'] = df_visitors['host'].apply(get_visitor_domain).map(lambda x: str(x))\n",
    "print('unique domains: ', df_visitors['domain'].unique())\n",
    "\n",
    "df_visitors['category'] = df_visitors['domain'].apply(categorise_visitor).map(lambda x: str(x))\n",
    "df_visitors.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top visitors (ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by and count number of visits\n",
    "df_top_visitors = df_visitors.groupby(['host'])['host'].count().sort_values(ascending=False)\n",
    "df_top_visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of visitors (distribution)\n",
    "1. Commercial\n",
    "2. Unknown - contains miscellaneous (e.g. CDN) request, unfamiliar domains, IP addresses\n",
    "3. Country - based on country-level top domain\n",
    "4. Education\n",
    "\n",
    "Interestingly, Government site visitors are on the 6th order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by and count number of visits\n",
    "df_top_visitors = df_visitors.groupby(['category'])['category'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "# horizontal bar plot\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "ax.barh(df_top_visitors['category'], df_top_visitors['count'])\n",
    "# add padding for better visibility\n",
    "ax.xaxis.set_tick_params(pad = 3)\n",
    "ax.yaxis.set_tick_params(pad = 5)\n",
    "# Add x, y gridlines\n",
    "ax.grid(b = True, color ='grey',\n",
    "        linestyle ='-.', linewidth = 0.5,\n",
    "        alpha = 0.2)\n",
    "# sort descending order\n",
    "ax.invert_yaxis()\n",
    "# Add annotation to bars\n",
    "for i in ax.patches:   \n",
    "    plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "             str(round((i.get_width()), 2)),\n",
    "             fontsize = 10, fontweight ='bold',\n",
    "             color ='grey') \n",
    "# Add Plot Title\n",
    "ax.set_title('Types of visitors') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visiting countries (represents only a SUBSET based on identifiable country top-level domain):\n",
    "1. Canada\n",
    "2. UK\n",
    "3. Japan\n",
    "4. Australia\n",
    "5. Germany\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visitor_country = df_visitors.loc[df_visitors['category'] == 'country']\n",
    "df_visitor_country = df_visitor_country.filter(['domain'])\n",
    "df_visitor_country = df_visitor_country.groupby(['domain'])['domain'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "df_visitor_country.head()\n",
    "\n",
    "# horizontal bar plot\n",
    "fig, ax = plt.subplots(figsize =(10, 20))\n",
    "ax.barh(df_visitor_country['domain'], df_visitor_country['count'])\n",
    "# add padding for better visibility\n",
    "ax.xaxis.set_tick_params(pad = 5)\n",
    "ax.yaxis.set_tick_params(pad = 10)\n",
    "# Add x, y gridlines\n",
    "ax.grid(b = True, color ='grey',\n",
    "        linestyle ='-.', linewidth = 0.5,\n",
    "        alpha = 0.2)\n",
    "# sort descending order\n",
    "ax.invert_yaxis()\n",
    "# Add annotation to bars\n",
    "for i in ax.patches:   \n",
    "    plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "             str(round((i.get_width()), 2)),\n",
    "             fontsize = 10, fontweight ='bold',\n",
    "             color ='grey') \n",
    "# Add Plot Title\n",
    "ax.set_title('Visitor Countries (subset only based on host domain)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe resulting from host analysis\n",
    "df_visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp\n",
    "* Add Month column\n",
    "* Add Day column\n",
    "* Add Hour column\n",
    "* Add Mins column\n",
    "* Nominal category: AM or PM\n",
    "* Nominal category: day of the week\n",
    "* Insight idea: show trends based on:\n",
    "    * busiest day of all time (within 2 months of 1995)\n",
    "    * most visited day in a week\n",
    "    * busiest hour in a day (can help inform the right time for website updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df for breaking down timestamp\n",
    "df_timestamp = pd.DataFrame(df_cleaned_logs['timestamp'])\n",
    "\n",
    "df_timestamp['date'] = df_timestamp['timestamp'].dt.date\n",
    "\n",
    "# extract month, date, hour, minutes, day of week\n",
    "df_timestamp['month'] = df_timestamp['timestamp'].dt.month\n",
    "df_timestamp['day'] = df_timestamp['timestamp'].dt.day\n",
    "df_timestamp['hour'] = df_timestamp['timestamp'].dt.hour\n",
    "df_timestamp['minutes'] = df_timestamp['timestamp'].dt.minute\n",
    "df_timestamp['dayofweek'] = df_timestamp['timestamp'].dt.day_of_week\n",
    "\n",
    "# Get a map to translate month\n",
    "m = dict(zip(range(12),list(calendar.month_name)))\n",
    "df_timestamp['month'] = df_timestamp['month'].map(m)\n",
    "\n",
    "# Get a map to translate to day of week\n",
    "d = dict(zip(range(7),list(calendar.day_name)))\n",
    "df_timestamp['dayofweek'] = df_timestamp['dayofweek'].map(d)\n",
    "\n",
    "# AM/PM category\n",
    "df_timestamp['dayperiod'] = df_timestamp['hour'].apply(lambda x: 'AM' if x < 12 else 'PM')\n",
    "\n",
    "# output of this will validate it looks right\n",
    "df_timestamp.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend Analysis\n",
    "\n",
    "The busiest days across the 2 month data set -- **TODO: try to see and explain what happened on those days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp_days = df_timestamp.groupby(['date'])['date'].count().reset_index(name='visits')\n",
    "\n",
    "# plot line chart\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "ax.plot(df_timestamp_days['date'], df_timestamp_days['visits'])\n",
    "\n",
    "# Major ticks every half-month\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(bymonthday=(1, 15)))\n",
    "ax.grid(True)\n",
    "ax.set_ylabel('visits')\n",
    "ax.set_title('Visit trends per day (during Jul & Aug 1995)', fontsize='medium')\n",
    "# Text in the x-axis.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%d/%b'))\n",
    "# Rotates and right-aligns the x labels so they don't crowd each other.\n",
    "for label in ax.get_xticklabels(which='major'):\n",
    "    label.set(rotation=30, horizontalalignment='right')\n",
    "\n",
    "#annotate pertinent dates in data with visits greater than 90k\n",
    "annotated_data = df_timestamp_days[df_timestamp_days['visits'] > 90000]\n",
    "# Annotate data points\n",
    "for index, row in annotated_data.iterrows():\n",
    "    ax.annotate(row['date'].strftime('%d/%b'), \n",
    "                xy=(row['date'], row['visits']), color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Days of the week access trends\n",
    "* Thursdays are the busiest on both months -- **TODO: try to analyse on the type of requests**\n",
    "* Weekends are relatively quieter than the weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp_days = df_timestamp.groupby(['dayofweek', 'month'])['dayofweek'].count().reset_index(name='visits')\n",
    "\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "ax = pd.pivot_table(df_timestamp_days.reset_index(),\n",
    "               index='dayofweek', columns='month', values='visits'\n",
    "              ).loc[day_order].plot(kind='bar')\n",
    "\n",
    "ax.set_ylabel('visits')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('Day of week visit trends (during Jul & Aug 1995)', fontsize='medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hours in a day access trends\n",
    "* Follows standard business hours activity (between 8am - 6pm)\n",
    "* Busiest late after lunch\n",
    "* Least activity on wee hours of the morning (4 - 6am) -- best time to do any patches\n",
    "    * however, worthwhile to do further digging on the sites frequently visited during these timeframe to aid update rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp_days = df_timestamp.groupby(['hour', 'month'])['hour'].count().reset_index(name='visits')\n",
    "\n",
    "ax = pd.pivot_table(df_timestamp_days.reset_index(),\n",
    "               index='hour', columns='month', values='visits'\n",
    "              ).plot(kind='line')\n",
    "\n",
    "ax.figure.set_size_inches(10, 6)\n",
    "ax.set_ylabel('visits')\n",
    "ax.set_xlabel('Hour of day')\n",
    "ax.set_title('Hour of day visit trends (during Jul & Aug 1995)', fontsize='medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe resulting from timestamp analysis\n",
    "df_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request\n",
    "* Request method\n",
    "* URL traversal path\n",
    "* Page OR Resource OR None\n",
    "* HTTP/1.0 is irrelevant (doesn't give insight because all requests use it for this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_logs['request'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_method(req):\n",
    "    try:\n",
    "        method = req.split(' ')[0]\n",
    "        #method = request[0]\n",
    "        #protocol = request[2]   # irrelevant and won't be used\n",
    "    except:\n",
    "        method = None\n",
    "    return method\n",
    "\n",
    "def parse_url(req):\n",
    "    try:\n",
    "        url = req.split(' ')[1]\n",
    "        #url = request[1]\n",
    "        #protocol = request[2]   # irrelevant and won't be used\n",
    "    except:\n",
    "        url = None\n",
    "    return url\n",
    "\n",
    "# create a new df for breaking down request\n",
    "df_request = pd.DataFrame(df_cleaned_logs['request'])\n",
    "\n",
    "df_request['reqmethod'] = df_request['request'].apply(parse_method).map(lambda x: str(x))\n",
    "df_request['requrl'] = df_request['request'].apply(parse_url).map(lambda x: str(x))\n",
    "\n",
    "# output of this will validate it looks right\n",
    "df_request.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_request['reqmethod'].unique()\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe resulting from request analysis\n",
    "df_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "* nominal refer to https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "    * 1xx: informational\n",
    "    * 2xx: success\n",
    "    * 3xx: redirect\n",
    "    * 4xx: client error\n",
    "    * 5xx: server error\n",
    "* more indicator may be needed to identify points for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = pd.DataFrame(\n",
    "    pd.to_numeric(df_cleaned_logs['response'], errors='coerce', downcast='integer'))\n",
    "\n",
    "df_response['response'] = df_response['response'].fillna(400).apply(np.int64)\n",
    "\n",
    "df_response['response'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log entries that have anomalous parsed http response are defaulted to 404 NOT FOUND. Justification - it reflects the user's likely experience that they were not able to retrieve the page they're looking for.\n",
    "\n",
    "Status codes 1, 2 and 3 are deemed to be successful, the rest are deemed as failure. \n",
    "* **TODO** Thought experiment for later improvement: for Status code 3 as fail - redirection introduces latency that could have an impact to user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_status(code):\n",
    "    try:\n",
    "        respstatus = HTTPStatus(code).phrase\n",
    "    except:\n",
    "        respstatus = HTTPStatus(404).phrase #None\n",
    "    return respstatus\n",
    "\n",
    "def get_response_class(code):\n",
    "    switcher = {\n",
    "        1: 'informational',\n",
    "        2: 'successful',\n",
    "        3: 'redirection',\n",
    "        4: 'client error',\n",
    "        5: 'server error',\n",
    "    }\n",
    "    code_index = int(str(code)[0])\n",
    "    respclass = switcher.get(code_index)\n",
    "    return respclass\n",
    "\n",
    "def get_response_result(code):\n",
    "    code_index = int(str(code)[0])\n",
    "    if code_index in [1, 2, 3]:\n",
    "        result = 'success'\n",
    "    else:\n",
    "        result = 'fail'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response['respstatus'] = df_response['response'].apply(get_response_status)\n",
    "df_response['respstatus'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response['respclass'] = df_response['response'].apply(get_response_class)\n",
    "df_response['respclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response['result'] = df_response['response'].apply(get_response_result)\n",
    "df_response['result'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of response codes across 2 month timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4,figsize=(20,20))\n",
    "\n",
    "# group by and count number of visits\n",
    "df_top_responses = df_response.groupby(['result'])['result'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "# horizontal bar plot\n",
    "labels = df_top_responses['result']\n",
    "values = df_top_responses['count'] / len(df_top_responses)\n",
    "ax1.pie(values, labels = labels, autopct = '%1.1f%%')\n",
    "ax1.set_title('Success/Fail response distribution')\n",
    "\n",
    "df_success_responses = df_response.loc[df_response['result'] == 'success']\n",
    "\n",
    "# group by and count number of visits\n",
    "df_success_responses = df_success_responses.groupby(['respstatus'])['respstatus'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "labels = df_success_responses['respstatus']\n",
    "values = df_success_responses['count'] / len(df_success_responses)\n",
    "ax2.pie(values,labels = labels, autopct = '%1.1f%%')\n",
    "ax2.set_title('Success response status distribution')\n",
    "\n",
    "df_fail_responses = df_response.loc[df_response['respclass'] == 'client error']\n",
    "\n",
    "# group by and count number of visits\n",
    "df_fail_responses = df_fail_responses.groupby(['respstatus'])['respstatus'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "labels = df_fail_responses['respstatus']\n",
    "values = df_fail_responses['count'] / len(df_fail_responses)\n",
    "ax3.pie(values, labels = labels, autopct = '%1.1f%%')\n",
    "ax3.set_title('Client error response status distribution')\n",
    "\n",
    "df_fail_responses = df_response.loc[df_response['respclass'] == 'server error']\n",
    "\n",
    "# group by and count number of visits\n",
    "df_fail_responses = df_fail_responses.groupby(['respstatus'])['respstatus'].count().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "labels = df_fail_responses['respstatus']\n",
    "values = df_fail_responses['count'] / len(df_fail_responses)\n",
    "ax4.pie(values,labels = labels, autopct = '%1.1f%%')\n",
    "ax4.set_title('Server error response status distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe resulting from response analysis\n",
    "df_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bytes\n",
    "* Clean values in bytes variable\n",
    "* Threshold indicator to build case for potential point for improvement - at this point, I'm unsure if basic  statistical distribution analysis is strong enough to make this threshold inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values are replaced with 0 as it is likely that no content was returned on the \n",
    "\n",
    "Small percentage of 0 content reponse - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bytes = pd.DataFrame(pd.to_numeric(df_cleaned_logs['bytes'], errors='coerce', downcast='integer'))\n",
    "\n",
    "df_bytes['bytes'] = df_bytes['bytes'].fillna(0)\n",
    "\n",
    "print('percentage of 0 content response: ', str(df_bytes[df_bytes['bytes'] == 0].count() / df_bytes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a wide range of returned bytes from 28 bytes to 6MB. Standard deviation is 76KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bytes[df_bytes['bytes'] > 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For readability, the unit standard is changed from bytes to KB: Standard deviation, 25th and 50th percentile are on KB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bytes = df_bytes / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using box plot to understand distribution, it became evident that there are a lot of identified outliers and that most of the content density is in less than 1KB range.\n",
    "\n",
    "There is a need to understand the distribution further and to potentially categorize the response content (e.g. S, M, L), which could potentially help with optimisation (e.g. CDN)\n",
    "\n",
    "Also, it is notable from the graph that there is a single outlier at the 6MB mark that have influenced the disparity between the initial quantiles and box plot. However, it is not prudent to remove this outlier at the moment, given that it may still be a valid content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return_bytes = df_bytes[df_bytes['bytes'] > 0]\n",
    "fig, ax = plt.subplots(figsize=(25,3))\n",
    "ax = df_return_bytes.boxplot(column='bytes', vert=False)\n",
    "ax.set_yticklabels(['kilobytes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in to the non-outlier fences that the box plot hand indicated to get a clearer view of what is defined as 'normal' with respect to data. NOTE: this is not implying normal distribution.\n",
    "\n",
    "It is also interesting to note that majority of the 'normal' requests is under 1KB (mode = 0.7KB).\n",
    "\n",
    "The mode and median are a better indicator of the 'normal' requests that can be expected, instead of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_25pct_return_bytes = df_return_bytes.quantile(0.25)['bytes']\n",
    "df_50pct_return_bytes = df_return_bytes.quantile(0.5)['bytes']\n",
    "df_75pct_return_bytes = df_return_bytes.quantile(0.75)['bytes']\n",
    "\n",
    "#df_1stqt_return_bytes = df_return_bytes[df_return_bytes['bytes'].le(df_25pct_return_bytes)].filter(['bytes'])\n",
    "#df_2ndqt_return_bytes = df_return_bytes[df_return_bytes['bytes'].gt(df_25pct_return_bytes) & df_return_bytes['bytes'].le(df_50pct_return_bytes)].filter(['bytes'])\n",
    "#df_3rdqt_return_bytes = df_return_bytes[df_return_bytes['bytes'].gt(df_50pct_return_bytes) & df_return_bytes['bytes'].le(df_75pct_return_bytes)].filter(['bytes'])\n",
    "\n",
    "iqr = df_75pct_return_bytes - df_25pct_return_bytes #Interquartile range\n",
    "fence_low = df_25pct_return_bytes - (1.5*iqr)\n",
    "fence_high = df_75pct_return_bytes + (1.5*iqr)\n",
    "\n",
    "print('iqr: ', iqr, ' low fence: ', fence_low, ' high fence: ', fence_high)\n",
    "bytes_mean = df_return_bytes.mean()['bytes']\n",
    "print('mean: ', bytes_mean)\n",
    "bytes_median = df_return_bytes.median()['bytes']\n",
    "print('median: ', bytes_median)\n",
    "bytes_mode = df_return_bytes.mode()['bytes'][0]\n",
    "print('mode:',  bytes_mode)\n",
    "\n",
    "fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, figsize=(25,6), gridspec_kw= {\"height_ratios\": (0.2, 1)})\n",
    "\n",
    "sns.boxplot(data=df_return_bytes, x=\"bytes\", ax=ax_box, showfliers=False)\n",
    "sns.histplot(df_return_bytes[df_return_bytes['bytes'] < fence_high], bins=np.arange(0, fence_high, 1), kde=True)\n",
    "\n",
    "ax_box.set_title('Bytes distribution without outliers')\n",
    "ax_box.set(xlabel='')\n",
    "ax_hist.set(xlabel='kilobytes')\n",
    "ax_hist.axvline(bytes_mean, color='b', linestyle='--', label='mean (%.2f kb)' % bytes_mean)\n",
    "ax_hist.axvline(bytes_median, color='g', linestyle='-', label='median (%.2f kb)' % bytes_median)\n",
    "ax_hist.axvline(bytes_mode, color='r', linestyle='-', linewidth=4, label='mode (%.2f kb)' % bytes_mode)\n",
    "max_normal = df_return_bytes[df_return_bytes['bytes'] < fence_high].max()['bytes']\n",
    "ax_hist.axvline(max_normal, color='gray', linestyle='--', label='max (%.2f kb)' % max_normal)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the perceived outliers? We are not going to remove them, just observe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df_return_bytes[df_return_bytes['bytes'] > fence_high]\n",
    "\n",
    "outliers_fence_low = df_outliers.min()['bytes'] \n",
    "outliers_fence_high = df_outliers.max()['bytes']\n",
    "\n",
    "obytes_mean = df_outliers.mean()['bytes']\n",
    "print('outliers mean: ', obytes_mean)\n",
    "obytes_median = df_outliers.median()['bytes']\n",
    "print('outliers median: ', obytes_median)\n",
    "obytes_mode = df_outliers.mode()['bytes'][0]\n",
    "print('outliers mode:',  obytes_mode)\n",
    "\n",
    "fig, oax_hist = plt.subplots(figsize=(25,6))\n",
    "\n",
    "oax_hist = sns.histplot(df_outliers, bins=np.arange(outliers_fence_low, outliers_fence_high, 50))\n",
    "\n",
    "oax_hist.set(xlabel='kilobytes')\n",
    "oax_hist.axvline(obytes_mean, color='b', linestyle='--', label='mean (%.2f kb)' % obytes_mean)\n",
    "oax_hist.axvline(obytes_median, color='g', linestyle='-', label='median (%.2f kb)' % obytes_median)\n",
    "oax_hist.axvline(obytes_mode, color='r', linestyle='-', linewidth=4, label='mode (%.2f kb)' % obytes_mode)\n",
    "oax_hist.axvline(outliers_fence_high, color='gray', linestyle='--', label='max (%.2f kb)' % outliers_fence_high)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now categorise bytes according to basic statistical distribution analysis, where the threshold of what's considered 'normal' and 'for observation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bytes['bytesaction'] = df_bytes['bytes'].apply(lambda x: 'normal' if x <= fence_high else 'for observation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe resulting from bytes analysis\n",
    "df_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relationship between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations (?)\n",
    "\n",
    "* **TODO** Think of how we can augment this with GPT to improve experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
